{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhoudsopgave\n",
    "1. [Inleiding](#Inleiding)\n",
    "2. [Doelen](#DoelenUitwerken)\n",
    "    1. [Hello World](#HelloWorld)\n",
    "    2. [Data Scrapyen van een Specifieke Site](#SpecifiekeSite)\n",
    "    3. [Data Opslaan in JSON of CSV via Items Container](#DataOpslaan)\n",
    "3. [Slot](#Slot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inleiding\n",
    "<a id='Inleiding'></a>\n",
    "## Wat is Scrapy Oversimplified\n",
    "Scrapy is een webcrawl library die kijkt naar de HTML/CSS code van een website en haalt daar alle relevante data uit. \n",
    "\n",
    "*Sidenote: sommige websites hebben */robots.txt files geupload. Dit zijn regels vanuit het bedrijf dat je deze pagina's niet mag bezoeken met webcrawlers. Het is mogelijk om deze regels te negeren, maar dat is niet helemaal kosher.*\n",
    "\n",
    "## Waarom is Scrapy interessant voor Tau Omega\n",
    "Als Tau Omega zelf informatie of data uit websites wil halen, dan kunnen we deze toel gebruiken.\n",
    "\n",
    "## Hoe moet je Scrapy gebruiken? (Programmeertaal/software)\n",
    "### Installatie\n",
    "Het is een library in Python. Gebruik `pip install scrapy` om het te installeren (schijnbaar wordt het aangeraden om in een virtueel env te installeren). [Kijk deze video voor hulp bij installeren.](https://www.youtube.com/watch?v=UoLu3PIkO2c&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t&index=5)\n",
    "\n",
    "Dit zijn de stappen om het te installeren:\n",
    "1. Create virtual environment\n",
    "2. `pip install scrapy`\n",
    "3. In terminal/cmd: `cd <vul hier je working directory in>`\n",
    "4. In terminal/cmd: `scrapy startproject <vul hier naam van project in>`\n",
    "\n",
    "### Project folder\n",
    "Als het project geïnstalleerd is zou je de volgende project folder moeten hebben:\n",
    "Als voorbeeld gebruiken we de projectnaam `tutorial`\n",
    "```\n",
    ".\n",
    "|Project folder (git folder)\n",
    "+-- tutorial\n",
    "|   +-- scrapy.cfg\n",
    "|   +-- tutorial\n",
    "|       +-- __init__.py\n",
    "|       +-- items.py\n",
    "|       +-- middlewares.py\n",
    "|       +-- pipelines.py\n",
    "|       +-- settings.py\n",
    "|       +-- spiders (deze is erg belangrijk)\n",
    "|           +-- __init__.py\n",
    "+-- Andere files\n",
    "+-- README.md          \n",
    "```\n",
    "\n",
    "## Doelen\n",
    "- Hello World Scrappy maken\n",
    "- Van 1 specifieke site data halen\n",
    "- Data van een site opslaan\n",
    "- Van meerdere sites data halen\n",
    "- Die data verkrijgbaar zetten met een API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doel uitwerking\n",
    "<a id='DoelenUitwerken'></a>\n",
    "## Hello World maken\n",
    "<a id='HelloWorld'></a>\n",
    "Als hello world gebruiken we de website [quotes.toscrape.com](http://quotes.toscrape.com/). \n",
    "\n",
    "Begin met in de source code te kijken (Ctrl+U voor Chrome). Aan het begin van de source code staat dit stukje HTML:\n",
    "```html\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Quotes to Scrape</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
    "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
    "</head>\n",
    "```\n",
    "\n",
    "Tussen de `head` tags zie je op de tweede regel de tag `title`. Dit gaan we proberen te scrapen. \n",
    "### Code + stap voor stap uitleg\n",
    "Aller eerst moet je in de `spiders` map een pythonscript aanmaken. Laten we deze `HelloWorld.py` noemen.\n",
    "\n",
    "We maken een class aan die scrapy.Spider inherit. We geven deze class een naam en een lijst met url(s) mee. *Note: de variable en functie namen moeten precies overeenkomen. Scrapy verwacht de variabelen `name`, `start_urls` en de functie `parse`. Anders veroorzaakt dit misschien errors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'hello'\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract()\n",
    "        \n",
    "        yield {'titletext': title}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de functie `parse` krijgen we de response van de website. Daaruit halen we met de `css` attribute de titel uit de website. Dit door `response.css('title')` te gebruiken, zouden de hele HTML regel terug krijgen.\n",
    "```html\n",
    "<title>Quotes to Scrape</title>\n",
    "```\n",
    "Alleen zijn die tags niet interessant, dus gebruiken we `response.css('title::text')` om aan te geven dat we alleen de text willen. De text zetten we in een dictionary en returnen/yielden we.\n",
    "\n",
    "Om het script uit te voeren gebruiken we de volgende code. Dit kan ook in de terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "~/Scrapy_tutorial/tutorial$ scrapy crawl hello\n",
    "OUT:\n",
    "2019-07-23 16:03:59 [scrapy.utils.log] INFO: Scrapy 1.7.1 started (bot: tutorial)\n",
    "2019-07-23 16:04:00 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.1, Python 3.7.3 (default, Mar 27 2019, 22:11:17) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Linux-4.18.0-25-generic-x86_64-with-debian-buster-sid\n",
    "2019-07-23 16:04:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "2019-07-23 16:04:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/>\n",
    "{'titletext': ['Quotes to Scrape']}\n",
    "2019-07-23 16:04:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "        .\n",
    "        .\n",
    "        .\n",
    " 'scheduler/dequeued/memory': 1,\n",
    " 'scheduler/enqueued': 1,\n",
    " 'scheduler/enqueued/memory': 1,\n",
    " 'start_time': datetime.datetime(2019, 7, 23, 14, 4, 1, 229999)}\n",
    "2019-07-23 16:04:01 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit werkt goed. Het is even zoeken in de terminal, maar het werkt. Het is ook mogelijk om via de scrapy shell te werken, maar dit is beter uit te leggen via video vorm. [Ik raad je aan om deze video te kijken voor de scrapy shell en de CSS Selector](https://www.youtube.com/watch?v=FQv-whbCfKs&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t&index=9). Dit maakt het webscraping ietsjes makkelijker om direct te kijken of je de juiste HTML tags hebt. De volgende code zal ik in de shell uitvoeren aangezien het maar een paar regels veranderd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Willen we de quotes lezen, dan gebruiken we de CSS Selector om de CSS attribute van de quotes op te halen. Hieruit lezen we af dat het gaat om de `.text` attribute.\n",
    "\n",
    "![CSS Selector lezen](img/CSS_Selector.png)\n",
    "\n",
    "Om dit op te halen in de shell voeren we eerst `scrapy shell \"http://quotes.toscrape.com/\"` uit. En vervolgens\n",
    "```shell\n",
    "IN: response.css(\".text::text\").extract()\n",
    "OUT: [\n",
    "'“The world as we have created it is a process of our thinking.\n",
    "  It cannot be changed without changing our thinking.”',\n",
    " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
    " '“There are only two ways to live your life. One is as though nothing is a miracle. \n",
    "   The other is as though everything is a miracle.”',\n",
    " '“The person, be it gentleman or lady, \n",
    " who has not pleasure in a good novel, must be intolerably stupid.”',\n",
    " \"“Imperfection is beauty, \n",
    " madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
    " '“Try not to become a man of success. Rather become a man of value.”',\n",
    " '“It is better to be hated for what you are than to be loved for what you are not.”',\n",
    " \"“I have not failed. I've just found 10,000 ways that won't work.”\",\n",
    " \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\",\n",
    " '“A day without sunshine is like, you know, night.”'\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is ook mogelijk om met Xpath te werken ipv CSS. Het enige wat veranderd is de syntax. In CSS deden we `response.css(\"title::text\")` om de titel uit een website halen en in xpath gebruiken we `response.xpath(//title/text()\")`. Willen we de quotes van de site, dan zouden we in CSS `response.css(\".text::text\")` gebruiken, maar in xpath is het `response.xpath(//span[@class = \"text\"]/text()).extract()`.\n",
    "\n",
    "Dit is overduidelijk een wat lastigere manier om hetzelfde te gebruiken, dus waarom zouden we dit doen? Het is mogelijk om xpath te gebruiken om links naar andere pagina's te vinden. Helemaal onderaan de pagina zit de **Next** knop. Als we deze link willen gebruiken vinden we (met de CSS Selector) dat de attribute `.next a` is, maar we willen de waarde van `href` weten. Hierdoor krijgen we\n",
    "\n",
    "```shell\n",
    "IN: response.css(\".next a\").xpath(\"@href\").extract()\n",
    "OUT: ['/page/2/']\n",
    "```\n",
    "\n",
    "Willen we gewoon alle links op een website, dan laten we `.next` weg.\n",
    "```shell\n",
    "IN: response.css(\"a\").xpath(\"@href\").extract()\n",
    "OUT: ['/',\n",
    " '/login',\n",
    " '/author/Albert-Einstein',\n",
    " '/tag/change/page/1/',\n",
    " '/tag/deep-thoughts/page/1/',\n",
    " '/tag/thinking/page/1/',\n",
    " '/tag/world/page/1/',\n",
    " '/author/J-K-Rowling',\n",
    " '/tag/abilities/page/1/',\n",
    " '/tag/choices/page/1/',\n",
    "         .\n",
    "         .\n",
    "         .\n",
    " '/tag/reading/',\n",
    " '/tag/friendship/',\n",
    " '/tag/friends/',\n",
    " '/tag/truth/',\n",
    " '/tag/simile/',\n",
    " 'https://www.goodreads.com/quotes',\n",
    " 'https://scrapinghub.com']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voorbeeld voor binnen Tau Omega\n",
    "Niets. Puur ennismaking met de tool.\n",
    "\n",
    "### Eventuele aanschaffing van software/hardware\n",
    "Niks. Pas wel op met het scrapen van bepaalde websites. Anders kunnen we een legal team aanschaffen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronnen\n",
    "\n",
    "- [Youtube tutorial](https://www.youtube.com/watch?v=ve_0h4Y8nuI&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t&index=1)\n",
    "\n",
    "- [http://quotes.toscrape.com/](http://quotes.toscrape.com/)\n",
    "\n",
    "- [Amazon books last 30 days](https://www.amazon.com/Books-Last-30-days/s?rh=n%3A283155%2Cp_n_publication_date%3A1250226011)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doel van een specifieke website halen\n",
    "<a id='SpecifiekeSite'></a>\n",
    "Wederom gebruiken we weer de website [http://quotes.toscrape.com/](http://quotes.toscrape.com/), maar omdat deze specifiek gemaakt is om webscraping te leren kies ik ook een uitdaging. Ik ga proberen de synoniemen van desalniettemin te vinden op [https://synoniemen.net/index.php?zoekterm=desalniettemin](https://synoniemen.net/index.php?zoekterm=desalniettemin).\n",
    "### Code + stap voor stap uitleg\n",
    "We maken een nieuw pythonscript aan genaamd `quotes_tutorial.py`. \n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/'\n",
    "    ]\n",
    "``` \n",
    "We geven een andere naam en zeggen welke url's er gescraped moeten worden. In deze class zetten we de functie `parse`. \n",
    "\n",
    "![inspector van website](img/quotes_div.png)\n",
    "\n",
    "Zoals je in de afbeelding ziet, zijn alle quotes, auteurs en tags per `div` op de site gezet. We kunnen de website doorlezen per `div.quote`.\n",
    "\n",
    "```python\n",
    "    def parse(self, response):\n",
    "        all_div_quotes = response.css(\"div.quote\")\n",
    "        \n",
    "        for quotes in all_div_quotes:\n",
    "            quote_text = quotes.css(\"span.text::text\").extract()\n",
    "            author = quotes.css(\".author::text\").extract()\n",
    "            tag = quotes.css(\".tag::text\").extract()\n",
    "\n",
    "            yield {\n",
    "                \"quote\": quote_text,\n",
    "                \"authors\": author,\n",
    "                \"tags\": tag\n",
    "            }\n",
    "```\n",
    "Per quote blok wordt de tekst, auteur en tag gelezen, en vervolgens in een dictionary gezet. Dit betekent dat je veel dictionaries krijgt. Het is ook mogelijk om alles in 1 dictionary te doen met de volgende code:\n",
    "\n",
    "```python\n",
    "    def parse(self, response):\n",
    "        all_div_quotes = response.css(\"div.quote\")\n",
    "        \n",
    "\n",
    "        quote_text = all_div_quotes.css(\"span.text::text\").extract()\n",
    "        author = all_div_quotes.css(\".author::text\").extract()\n",
    "        tag = all_div_quotes.css(\".tag::text\").extract()\n",
    "\n",
    "        yield {\n",
    "            \"quote\": quote_text,\n",
    "            \"authors\": author,\n",
    "            \"tags\": tag\n",
    "        }\n",
    "```\n",
    "\n",
    "### Synoniemen\n",
    "Als we alles de synoniemen website pakken, dan maken we eerst een nieuw projectje aan met `scrapy startproject synonym` met de spider `desalniettemin_synoniem.py` (excuus voor het wisselen tussen Engels en Nederlands). Laten we beginnen met de site te onderzoeken.\n",
    "\n",
    "![Synoniemen in de website met CSS Selector](img/synoniem_CSS_SELECTOR.png) \n",
    "\n",
    "Zoals je kan zien staan de synoniemen in een blok van `.alssynoniemtabel` met daarin nog een blok van `.nowrap`.\n",
    "\n",
    "![Synoniemen van synoniemen](img/synoniemen_van_synoniemen.png)\n",
    "\n",
    "De synoniemen van de synoniemen staan ook in `.alssynoniemtabel`, maar dan in een ander blok genaamd `dd`. Overigens wordt **Desalniettemin** niet meegenomen omdat die dikgedrukt is. \n",
    "\n",
    "Dan krijgen we de volgende code:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'synoniem'\n",
    "    start_urls = [\n",
    "        'https://synoniemen.net/index.php?zoekterm=desalniettemin'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "                \n",
    "        all_synonym = response.css(\".alssynoniemtabel .nowrap\")\n",
    "        syno_desc = response.css(\".alssynoniemtabel dd\")\n",
    "        \n",
    "        for synonym, desc in zip(all_synonym, syno_desc):\n",
    "            syno = synonym.css(\"a::text\").extract()\n",
    "            descList = desc.css(\"a::text\").extract()\n",
    "            \n",
    "            yield {\n",
    "                \"Synonym\": syno,\n",
    "                \"Description\": descList\n",
    "            }\n",
    "```\n",
    "Dit uitvoeren in de Terminal geeft:\n",
    "\n",
    "```shell\n",
    "~/Scrapy_tutorial/synonym$ scrapy crawl synoniem\n",
    "2019-07-24 11:01:46 [scrapy.utils.log] INFO: Scrapy 1.7.1 started (bot: synonym)\n",
    "            .\n",
    "            .\n",
    "            .\n",
    "2019-07-24 11:01:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://synoniemen.net/index.php?zoekterm=desalniettemin>\n",
    "{'Synonym': ['desondanks'], 'Description': ['afgezien daarvan', 'dat daargelaten', 'evenwel', 'hoe dan ook', 'niettemin', 'nochtans', 'ondanks alles', 'ondanks dat', 'toch']}\n",
    "2019-07-24 11:01:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://synoniemen.net/index.php?zoekterm=desalniettemin>\n",
    "{'Synonym': ['niettemin'], 'Description': ['desniettegenstaande', 'desondanks', 'echter', 'evengoed', 'evenwel', 'intussen', 'nochtans', 'ondertussen', 'toch']}\n",
    "2019-07-24 11:01:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://synoniemen.net/index.php?zoekterm=desalniettemin>\n",
    "{'Synonym': ['toch'], 'Description': ['alleen', 'desondanks', 'echter', 'evenwel', 'niettemin', 'nochtans', 'ondanks alles']}\n",
    "2019-07-24 11:01:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://synoniemen.net/index.php?zoekterm=desalniettemin>\n",
    "{'Synonym': ['evenwel'], 'Description': ['desniettegenstaande', 'echter', 'maar', 'niettemin', 'nochtans', 'toch']}\n",
    "2019-07-24 11:01:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://synoniemen.net/index.php?zoekterm=desalniettemin>\n",
    "{'Synonym': ['nochtans'], 'Description': ['desondanks', 'echter']}\n",
    "2019-07-24 11:01:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "            .\n",
    "            .\n",
    "            .\n",
    " 'scheduler/enqueued/memory': 1,\n",
    " 'start_time': datetime.datetime(2019, 7, 24, 9, 1, 46, 462213)}\n",
    "2019-07-24 11:01:46 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voorbeeld voor binnen Tau Omega\n",
    "Het is mogelijk om ook bijvoorbeeld beursdata in te lezen hiermee. Voor een demo zou het ook heel leuk zijn om bijvoorbeeld [r/watches](https://www.reddit.com/r/watches) demo te maken.\n",
    "\n",
    "### Eventuele aanschaffing van software/hardware\n",
    "[-]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronnen\n",
    "\n",
    "- [http://quotes.toscrape.com/](http://quotes.toscrape.com/)\n",
    "\n",
    "- [https://synoniemen.net/index.php?zoekterm=desalniettemin](https://synoniemen.net/index.php?zoekterm=desalniettemin)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data van een site opslaan\n",
    "<a id='DataOpslaan'></a>\n",
    "Dit lijkt misschien een erg triviaal stukje, maar er zit meer achter dan de dictionary in een dataframe zetten. We moeten dit namelijk doen via Scrapy Items containers. \n",
    "\n",
    "### Code + stap voor stap uitleg\n",
    "We maken in `~git_repo_folder/tutorial/tutorial/spider` een nieuwe file aan genaamd `quotes_items.py` met de volgende code:\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes_items'\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        all_div_quotes = response.css(\"div.quote\")\n",
    "        \n",
    "        for quotes in all_div_quotes:\n",
    "            title = quotes.css(\"span.text::text\").extract()\n",
    "            author = quotes.css(\".author::text\").extract()\n",
    "            tag = quotes.css(\".tag::text\").extract()\n",
    "```\n",
    "Dit is bijna hetzelfde als `quote_tutorial.py`, maar is `name` anders en de yield is ook weg gehaald. We gaan een paar dingen toevoegen en veranderen.\n",
    "\n",
    "Ten eerste gaan we in `items.py` werken. In het script van quotes halen we 3 variabelen eruit die we wilen opslaan. Dit zijn `title`, `author`, en `tag`. Deze variabelen moeten we opslaan in een `scrapy.Fields()` class. Dit wordt gedaan in `items.py`. Dit moet er als volgt uitzien:\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class TutorialItem(scrapy.Item):\n",
    "    # Definieer de variabelen die je wil opslaan\n",
    "    title = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    tag = scrapy.Field() \n",
    "```\n",
    "\n",
    "Deze class moet je importeren in je spider bestand met de volgende code:\n",
    "\n",
    "```python\n",
    "from ..items import TutorialItem\n",
    "```\n",
    "\n",
    "Daarnaast moet een instance gemaakt worden waar de variabelen toegekent worden. Wanneer dit is gedaan moet je de items container yielden. Dit is de code:\n",
    "```python\n",
    "import scrapy\n",
    "from ..items import TutorialItem\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes_items'\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Create instance\n",
    "        items = TutorialItem()\n",
    "        \n",
    "        all_div_quotes = response.css(\"div.quote\")\n",
    "        \n",
    "        for quotes in all_div_quotes:\n",
    "            title = quotes.css(\"span.text::text\").extract()\n",
    "            author = quotes.css(\".author::text\").extract()\n",
    "            tag = quotes.css(\".tag::text\").extract()\n",
    "            \n",
    "            # add data to items container\n",
    "            items[\"title\"] = title\n",
    "            items[\"author\"] = author\n",
    "            items[\"tag\"] = tag\n",
    "            \n",
    "            # return items\n",
    "            yield items\n",
    "```\n",
    "\n",
    "### Voorbeeld voor binnen Tau Omega\n",
    "Het is natuurlijk handig om de data die je van een website haalt, ook op te slaan. \n",
    "\n",
    "### Eventuele aanschaffing van software/hardware\n",
    "[-]\n",
    "\n",
    "### Bronnen\n",
    "[-]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in SQLite 3 DB zetten\n",
    "<a id= \"SQLiteDB\"></a>\n",
    "\n",
    "In dit stukje gaan we te werk met SQLite3 database. Dit is makkelijk te gebruiken en je hoeft niks te installeren. \n",
    "\n",
    "### Code + Stap voor Stap uitleg\n",
    "Als eerste moeten we graven in de `settings.py` file en opzoek gaan naar de voglende code:\n",
    "```python\n",
    "# ITEM_PIPELINES = {\n",
    "#     'tutorial.pipelines.TutorialPipeline': 300,\n",
    "# }\n",
    "```\n",
    "Deze mag je uncommenten. Dit zorgt er voor dat elke keer wanneer er een item *geyield* wordt, de `TutorialPipeline` class wordt aangeroepen. Deze staat in de `pipelines.py` file. Via deze file gaan we de items van de webpagina in een sqlite3 database zetten. \n",
    "\n",
    "Generieke code om een database aan te maken en iets toe te voegen is het volgende:\n",
    "```python\n",
    "import sqlite3\n",
    "\n",
    "# Maak verbinding met database\n",
    "# Als de db nog niet bestaat, dan wordt die aangemaakt\n",
    "conn = sqlite3.connect(\"myquotes.db\")\n",
    "curr = conn.cursor()\n",
    "\n",
    "# Comment this section after running!\n",
    "# Maak de tabel aan met de cursor\n",
    "# definieer de tabel namen en de soort datatype\n",
    "curr.execute(\"\"\"\n",
    "CREATE TABLE quotes_tb(\n",
    "title text,\n",
    "author text,\n",
    "tag text\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Voeg wat waarde toe aan de tabel\n",
    "curr.execute(\"\"\"\n",
    "INSERT INTO quotes_tb VALUES (\"Python is awesome!\", \"BuildWithPython\", \"Python\")\n",
    "             \"\"\")\n",
    "\n",
    "# Commit alle veranderingen en sluit de verbinding met de database\n",
    "conn.commit()\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "Mocht je de inhoud willen zien van deze database, dan kan je makkelijk [sqlite online](https://sqliteonline.com/) gebruiken. Via `File->Open DB->quote_tb` kan je de tabel openen. \n",
    "\n",
    "We willen dit in de pipeline zetten. Daarom moeten we de volgende functies toevoegen aan de class.\n",
    "```python\n",
    "def create_connection(self):\n",
    "    self.conn = sqlite3.connect(\"myquotes.db\")\n",
    "    self.curr = self.conn.cursor()\n",
    "    \n",
    "def create_table(self):\n",
    "    # Verwijder de tabel als die al is gemaakt.\n",
    "    self.curr.execute(\"\"\"DROP TABLE IF EXISTS quotes_tb\"\"\")\n",
    "    self.curr.execute(\"\"\"\n",
    "            CREATE TABLE quotes_tb(\n",
    "            title text, \n",
    "            author text,\n",
    "            tag text\n",
    "            )\n",
    "            \"\"\")\n",
    "```\n",
    "Deze functies moeten elke keer gemaakt worden wanneer de instance gemaakt wordt. Dus moeten we de volgende functie maken:\n",
    "```python\n",
    "def __init__(self):\n",
    "    self.create_connection()\n",
    "    self.create_table()\n",
    "```\n",
    "\n",
    "Dan kunnen we een functie maken, genaamd `store_db()` waarin we de waarde uit een item container toevoegen aan de database. Dit doen we met de volgende code:\n",
    "```python\n",
    "def store_db(self, item):\n",
    "    self.curr.execute(\"\"\"\n",
    "            INSERT INTO quotes_tb values(?,?,?)\n",
    "            \"\"\",(\n",
    "                item[\"title\"][0],\n",
    "                item[\"author\"][0],\n",
    "                item[\"tag\"][0]\n",
    "                )\n",
    "                     )\n",
    "    self.conn.commit()\n",
    "```\n",
    "\n",
    "Nu alle ingredienten zijn gesneden, kunnen we nu gaan koken. Gelukkig zijn we een *one pot stew* aan het maken. In de functie `process_items()` moeten we de functie `store_db()` aanroepen. Met als resultaat:\n",
    "```python\n",
    "def process_item(self, item, spider):\n",
    "    self.store_db(item)\n",
    "    print(\"\\n\\nItem succesfully stored into database\")\n",
    "    return item\n",
    "```\n",
    "\n",
    "Als we dit uitvoeren krijgen we:\n",
    "```shell\n",
    "~/Scrapy_tutorial/tutorial> scrapy crawl quotes items\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data van meerdere sites halen\n",
    "<a id = \"MeerdereSites\"></a>\n",
    "\n",
    "### Code + Stap voor Stap uitleg\n",
    "sqlite3 example maken\n",
    "\n",
    "sqlite online laten zien\n",
    "\n",
    "### Voorbeeld binnen Tau Omega\n",
    "\n",
    "### Eventuele aanschaffing van software/hardware\n",
    "\n",
    "### Bronnen\n",
    "- [https://sqliteonline.com/](https://sqliteonline.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slot\n",
    "<a id='Slot'></a>\n",
    "## Is het bruikbaar/nuttig\n",
    "\n",
    "## Nieuwe dingen de je bent tegengekomen die onderzocht moeten worden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maak een presentatie\n",
    "- Workshop of Presentatie?\n",
    "- Presenteren op: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergeet niet je document naar pdf te exporteren en in de dropbox map te zetten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
