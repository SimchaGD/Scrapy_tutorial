{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inleiding\n",
    "## Wat is Scrapy Oversimplified\n",
    "Scrapy is een webcrawl library die kijkt naar de HTML/CSS code van een website en haalt daar alle relevante data uit. \n",
    "\n",
    "*Sidenote: sommige websites hebben */robots.txt files geupload. Dit zijn regels vanuit het bedrijf dat je deze pagina's niet mag bezoeken met webcrawlers. Het is mogelijk om deze regels te negeren, maar dat is niet helemaal kosher.*\n",
    "\n",
    "## Waarom is Scrapy interessant voor Tau Omega\n",
    "Als Tau Omega zelf informatie of data uit websites wil halen, dan kunnen we deze toel gebruiken.\n",
    "\n",
    "## Hoe moet je Scrapy gebruiken? (Programmeertaal/software)\n",
    "### Installatie\n",
    "Het is een library in Python. Gebruik `pip install scrapy` om het te installeren (schijnbaar wordt het aangeraden om in een virtueel env te installeren). [Kijk deze video voor hulp bij installeren.](https://www.youtube.com/watch?v=UoLu3PIkO2c&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t&index=5)\n",
    "\n",
    "Dit zijn de stappen om het te installeren:\n",
    "1. Create virtual environment\n",
    "2. `pip install scrapy`\n",
    "3. In terminal/cmd: `cd <vul hier je working directory in>`\n",
    "4. In terminal/cmd: `scrapy startproject <vul hier naam van project in>`\n",
    "\n",
    "### Project folder\n",
    "Als het project geïnstalleerd is zou je de volgende project folder moeten hebben:\n",
    "Als voorbeeld gebruiken we de projectnaam `tutorial`\n",
    "```\n",
    ".\n",
    "|Project folder (git folder)\n",
    "├── tutorial\n",
    "|   ├── scrapy.cfg\n",
    "|   └── tutorial\n",
    "|       ├── __init__.py\n",
    "|       ├── items.py\n",
    "|       ├── middlewares.py\n",
    "|       ├── pipelines.py\n",
    "|       ├── settings.py\n",
    "|       └── spiders (deze is erg belangrijk)\n",
    "|           └── __init__.py\n",
    "├── Andere files\n",
    "└── README.md          \n",
    "```\n",
    "\n",
    "## Doelen\n",
    "- Hello World Scrappy maken\n",
    "- Van 1 specifieke site data halen\n",
    "- Van meerdere sites data halen\n",
    "- Die data verkrijgbaar zetten met een API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doel uitwerking\n",
    "## Hello World maken\n",
    "Als hello world gebruiken we de website [quotes.toscrape.com](http://quotes.toscrape.com/). \n",
    "\n",
    "Begin met in de source code te kijken (Ctrl+U voor Chrome). Aan het begin van de source code staat dit stukje HTML:\n",
    "```html\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Quotes to Scrape</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
    "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
    "</head>\n",
    "```\n",
    "\n",
    "Tussen de `head` tags zie je op de tweede regel de tag `title`. Dit gaan we proberen te scrapen. \n",
    "### Code + stap voor stap uitleg\n",
    "Aller eerst moet je in de `spiders` map een pythonscript aanmaken. Laten we deze `HelloWorld.py` noemen.\n",
    "\n",
    "We maken een class aan die scrapy.Spider inherit. We geven deze class een naam en een lijst met url(s) mee. *Note: de variable en functie namen moeten precies overeenkomen. Scrapy verwacht de variabelen `name`, `start_urls` en de functie `parse`. Anders veroorzaakt dit misschien errors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract()\n",
    "        \n",
    "        yield {'titletext': title}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de functie `parse` krijgen we de response van de website. Daaruit halen we met de `css` attribute de titel uit de website. Dit door `response.css('title')` te gebruiken, zouden de hele HTML regel terug krijgen.\n",
    "```html\n",
    "<title>Quotes to Scrape</title>\n",
    "```\n",
    "Alleen zijn die tags niet interessant, dus gebruiken we `response.css('title::text')` om aan te geven dat we alleen de text willen. De text zetten we in een dictionary en returnen/yielden we.\n",
    "\n",
    "Om het script uit te voeren gebruiken we de volgende code. Dit kan ook in de terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-23 12:11:25 [scrapy.utils.log] INFO: Scrapy 1.7.1 started (bot: tutorial)\n",
      "2019-07-23 12:11:25 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.1, Python 3.7.3 (default, Mar 27 2019, 22:11:17) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Linux-4.18.0-25-generic-x86_64-with-debian-buster-sid\n",
      "2019-07-23 12:11:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "2019-07-23 12:11:25 [scrapy.extensions.telnet] INFO: Telnet Password: 7a4888a359d73a59\n",
      "2019-07-23 12:11:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-07-23 12:11:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-07-23 12:11:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-07-23 12:11:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-07-23 12:11:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-07-23 12:11:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-07-23 12:11:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-07-23 12:11:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
      "2019-07-23 12:11:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: None)\n",
      "2019-07-23 12:11:25 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/>\n",
      "{'titletext': ['Quotes to Scrape']}\n",
      "2019-07-23 12:11:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-07-23 12:11:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 446,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 2701,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 0.412133,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 7, 23, 10, 11, 25, 945954),\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 54120448,\n",
      " 'memusage/startup': 54120448,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 7, 23, 10, 11, 25, 533821)}\n",
      "2019-07-23 12:11:25 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit werkt goed. Het is even zoeken in de terminal, maar het werkt. Het is ook mogelijk om via de scrapy shell te werken, maar dit is beter uit te leggen via video vorm. [Ik raad je aan om deze video te kijken voor de scrapy shell en de CSS Selector](https://www.youtube.com/watch?v=FQv-whbCfKs&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t&index=9). Dit maakt het webscraping ietsjes makkelijker om direct te kijken of je de juiste HTML tags hebt. De volgende code zal ik in de shell uitvoeren aangezien het maar een paar regels veranderd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Willen we de quotes lezen, dan gebruiken we de CSS Selector om de CSS attribute van de quotes op te halen. Hieruit lezen we af dat het gaat om de `.text` attribute.\n",
    "![CSS Selector lezen](img/CSS_Selector.png)\n",
    "\n",
    "Om dit op te halen in de shell voeren we eerst `scrapy shell \"http://quotes.toscrape.com/\"` uit. En vervolgens\n",
    "```shell\n",
    "IN: response.css(\".text::text\").extract()\n",
    "OUT: ['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
    " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
    " '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',\n",
    " '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”',\n",
    " \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
    " '“Try not to become a man of success. Rather become a man of value.”',\n",
    " '“It is better to be hated for what you are than to be loved for what you are not.”',\n",
    " \"“I have not failed. I've just found 10,000 ways that won't work.”\",\n",
    " \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\",\n",
    " '“A day without sunshine is like, you know, night.”']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is ook mogelijk om met Xpath te werken ipv CSS. Het enige wat veranderd is de syntax. In CSS deden we `response.css(\"title::text\")` om de titel uit een website halen en in xpath gebruiken we `response.xpath(//title/text()\")`. Willen we de quotes van de site, dan zouden we in CSS `response.css(\".text::text\")` gebruiken, maar in xpath is het `response.xpath(//span[@class = \"text\"]/text()).extract()`.\n",
    "\n",
    "Dit is overduidelijk een wat lastigere manier om hetzelfde te gebruiken, dus waarom zouden we dit doen? Het is mogelijk om xpath te gebruiken om links naar andere pagina's te vinden. Helemaal onderaan de pagina zit de **Next** knop. Als we deze link willen gebruiken vinden we (met de CSS Selector) dat de attribute `.next a` is, maar we willen de waarde van `href` weten. Hierdoor krijgen we\n",
    "\n",
    "```shell\n",
    "IN: response.css(\".next a\").xpath(\"@href\").extract()\n",
    "OUT: ['/page/2/']\n",
    "```\n",
    "\n",
    "Willen we gewoon alle links op een website, dan laten we `.next` weg.\n",
    "```shell\n",
    "IN: response.css(\"a\").xpath(\"@href\").extract()\n",
    "OUT: ['/',\n",
    " '/login',\n",
    " '/author/Albert-Einstein',\n",
    " '/tag/change/page/1/',\n",
    " '/tag/deep-thoughts/page/1/',\n",
    " '/tag/thinking/page/1/',\n",
    " '/tag/world/page/1/',\n",
    " '/author/J-K-Rowling',\n",
    " '/tag/abilities/page/1/',\n",
    " '/tag/choices/page/1/',\n",
    "         .\n",
    "         .\n",
    "         .\n",
    " '/tag/reading/',\n",
    " '/tag/friendship/',\n",
    " '/tag/friends/',\n",
    " '/tag/truth/',\n",
    " '/tag/simile/',\n",
    " 'https://www.goodreads.com/quotes',\n",
    " 'https://scrapinghub.com']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voorbeeld voor binnen Tau Omega\n",
    "Niets. Puur ennismaking met de tool.\n",
    "\n",
    "### Eventuele aanschaffing van software/hardware\n",
    "Niks. Pas wel op met het scrapen van bepaalde websites. Anders kunnen we een legal team aanschaffen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronnen\n",
    "\n",
    "- [Youtube tutorial](https://www.youtube.com/watch?v=ve_0h4Y8nuI&list=PLhTjy8cBISEqkN-5Ku_kXG4QW33sxQo0t&index=1)\n",
    "\n",
    "- [http://quotes.toscrape.com/](http://quotes.toscrape.com/)\n",
    "\n",
    "- [Amazon books last 30 days](https://www.amazon.com/Books-Last-30-days/s?rh=n%3A283155%2Cp_n_publication_date%3A1250226011)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doel van een specifieke website halen\n",
    "Wederom gebruiken we weer de website [http://quotes.toscrape.com/](http://quotes.toscrape.com/), maar omdat deze specifiek gemaakt is om webscraping te leren kies ik ook een uitdaging. Ik ga de opleidingen van fontys scrapen met de site [https://fontys.nl/Studeren/Opleidingen.htm](https://fontys.nl/Studeren/Opleidingen.htm)\n",
    "### Code + stap voor stap uitleg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voorbeeld voor binnen Tau Omega\n",
    "\n",
    "### Eventuele aanschaffing van software/hardware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slot\n",
    "## Is het bruikbaar/nuttig\n",
    "\n",
    "## Nieuwe dingen de je bent tegengekomen die onderzocht moeten worden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maak een presentatie\n",
    "- Workshop of Presentatie?\n",
    "- Presenteren op: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergeet niet je document naar pdf te exporteren en in de dropbox map te zetten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
